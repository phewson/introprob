\documentclass{article}
%\usepackage[pdftex]{hyperref}
\usepackage{textcomp}
\usepackage[pdftex]{graphicx}
\usepackage{amsmath,amssymb}
\usepackage[pdftex,designi,nodirectory,navibar,usesf]{web} %removed usetemplates
\usepackage[pdftex]{exerquiz}
\usepackage{synttree}
\usepackage{keystroke}
%\template{../img/logo.pdf}
\definecolor{Blue}{cmyk}{1,1,0.,0.}
\definecolor{plum}{cmyk}{.5,1,0.,0.}
\definecolor{orange}{cmyk}{0.5,0.51,1,0}
\definecolor{Red}{rgb}{1.0,0.2,0.25}
\definecolor{Green}{rgb}{.1,.6,.3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Probability Crash Course: \\ Important Discrete distributions}
\author{Paul Hewson}
\authorColor{blue}
\date{7th September 2012}


\newtheorem{df}{Definition}

\university{
\null\hspace{-.6cm}\raisebox{.25cm}{\includegraphics[width=1.5cm]{logo}}
\hspace{.5cm}\raisebox{.7cm}
{School of Mathematics and Statistics}\hspace{1cm}
%\includegraphics[width=0.9cm]{../EBP/img/logo.pdf}
 }


\email{paul.hewson@plymouth.ac.uk}
\version{0.2} 
\copyrightyears{2008}


\optionalPageMatter{\vfill
  \begin{center}
  \fcolorbox{blue}{webyellow}{
   \begin{minipage}{.67\linewidth}
   \noindent \textcolor{red}{\textbf{Overview:}}
   This webfile is designed as a revision aid to some introductory
   concepts in probability.   It is intended to
   supplement a formal encouter with a text book or a set of
   lectures.   These notes are meant to be slightly interactive, mysterious
   green dots, squares and boxes appear which you can click on to
   answer questions and check solutions.
  \end{minipage}}
  \end{center}}

\begin{document}
\SweaveOpts{concordance=TRUE}


\maketitle

%tableofcontents


\section{Some important examples of discrete distributions}

This is set out in Section 5.1 of Grinstead and Snell (page 183)

\subsection{Bernoulli trials}

{\color{green}By convention we write $Z \sim Bern(\pi)$}


Repeated \emph{independent} trials are called Bernoulli if:
\begin{itemize}
\item There are two possible outcomes (events) conventionally denoted success
  and failure.   More broadly we should think of this as an indicator random variable where:

\begin{displaymath}
Z = \left\{ \begin{array}{rrr} 1 & \mbox{ if } & \mbox{ Event happens} \\ 
0 & & \mbox{Otherwise} \end{array} \right.
\end{displaymath}
\item The probabilities ($P[X=1]=p$ and $P[X=0]=(1-p)=q$) remain the same from trial
  to trial
\end{itemize}

An example of a Bernoulli trial is a coin tossing experiment.   Also,
if we have a stable production process the probability of selecting a
defective item might be Bernoulli.   Can you suggest some other examples?

\newpage

If we denote the parameter by $\pi$, we have:

\begin{displaymath}
\left\{ \begin{array}{rrl} p[Z=1] & =& \pi\\
p[Z=0] & =& 1-\pi
\end{array} \right.
\end{displaymath}

We can simply check that $\sum_{x=0}^1 Pr[X=i] = 1$ as we have $\pi$ = $1-\pi=1$.

The mean of $Z$ i.e. $E[Z]$ is found as: 
\begin{displaymath}
1 \times \pi + 0 \times (1-\pi) = \pi
\end{displaymath}

To find the variance, we first need 
\begin{displaymath}
E[Z^2] = 1^2 \times \pi + 0^2 \times 1-\pi = \pi
\end{displaymath}, so 

\begin{displaymath}
E[Z^2]-E[Z]^2 = \pi - \pi^2 = \pi(1 - \pi).
\end{displaymath}

\newpage

\subsubsection{Indicator random variables}

Note that the probability that an event $A$ happens, i.e. $P[A]=\pi$ is the same as the expectation $E[Z]$.

So note that in this indicator variable example we have a link where:

\begin{displaymath}
E[Z] = P[A]
\end{displaymath}

Demonstrating a close relationship between expectation and probability.

\subsection{Binomial distribution}

{\color{green}By convention we write $X \sim B(n, \pi)$}


Suppose we have a sequence of $n$ Bernoulli trials, with Bernoulli
random variables $Z_1, Z_2, \ldots, Z_n$ (all taking on values 1 or
0).   The random variable $X=Z_1 + Z_2 + \ldots + Z_n$ essentially
denotes the number of \emph{successes} amongst the $n$ Bernoulli
trials.

We could alternatively just think of it as the number of \emph{successes} in $n$ trials and forget the Bernoulli link.

This value has a formula as follows:

\begin{displaymath}
f(x) = p[X=x] = {n \choose x} \pi^x (1-\pi)^{n-x} \mbox{ for }
x=0,1,2,\ldots,n
\end{displaymath}

\newpage

Let's think a little about why.   Assuming that $P[Success]=p$ and $P[Failure] = (1-p) = q$, consider the probability of the following event:

\begin{tabular}{rrrrrr}
F & S & F & F & S & S \\
q & p & q & q & p & p \\
\end{tabular}

As the individual events are independent, we can just multiply the probabilities and so we have $q \times p \times q \times q \times p \times p = p^3q^3$.   Or if we said we had $n$ trials and $x$ successes, with $n-x$ failures we have $p^xq^{(n-x)}$.

The only problem here is that this is the probability of getting that particular ordering of Failures and Successes.   We could have got 3 successes from (S, S, S, F, F, F), or a number of other orderings.   How many orderings.   We did that already, it's $\binom{n}{x}$.   So, the probability of getting $x$ successes in $n$ trials is going to be:

\begin{displaymath}
P[X=x] = \binom{n}{x}p^xq^{n-x}
\end{displaymath}
as required.

\newpage

We can check that this is non-negative.   What about summing to one.   Well:

\begin{displaymath}
\sum_{x=0}^n \binom{n}{x} p^xq^{n-x} = (p+q)^n
\end{displaymath}
by the Binomial theorem.  And for this particular problem, we know that $(p+q)=1$, so it is a pmf which obeys the second Kolmogorov Axiom.

\newpage

Find $E[X]$ the hard way:

\begin{displaymath}
E[X] = \sum_{x=0}^n x \binom{n}{x} p^xq^{n-x}
\end{displaymath}
The first term is $0$ at $x=0$ so start the summation at $x=1$ and use Committee Chairman trick to change the permutation counter:
\begin{displaymath}
E[X] = \sum_{x=1}^n  n \binom{n-1}{x-1} p^xq^{n-x} 
\end{displaymath}
Now take that $n$ (and one $p$) outside the summation
\begin{displaymath}
E[X] = np \sum_{x=1}^n   \binom{n-1}{x-1} p^{x-1}q^{n-x}
\end{displaymath}

Now just let $j=x-1$ (and $m=n-1$ if you worry about that sort of thing.   The term on the right becomes $1$ so we have
\begin{displaymath}
E[X] = 1
\end{displaymath}

\newpage

Find $E[X]$ the easy way.

We said it's the sum of $n$ iid Bernoulli r.v.s.   So by linearity:
\begin{displaymath}
E[X] = E[Z_1 + Z_2 + \cdots + Z_n] = E[Z_1] + E[Z_2] + \cdots + E[Z_n]
\end{displaymath}
We said before $E[Z] = p$, so this is just $np$.   We can find $E[X^2]$ in the same way and hence confirm:

\fbox{For $X \sim Binomial(n,p)$: $E[X] = np$ and $V[X]=npq$.}


Nowadays, it is pretty straightforward to compute these values, but
once upon a time it was conventional to work with tables of
probabilities.   Let's work one example through.

\newpage

\useBeginQuizButton
\useEndQuizButton 

\begin{quiz}{binomial}
Consider a computer network with $n=5$ computers.   The probability
that a machine develops a fault in any given week is $\pi=0.1$.   We
wish to examine $X$, the number of computers that are faulty in our
sample of 5.

In formal notation, we say that $X \sim Binomial(5, 0.1)$ (or $X \sim
B(5, 0.1)$).   Find the following:
\begin{questions}
\item\PTs{1} The probability that no computers fail  (2d.p.)\\
\RespBoxTxt{0}{0}[b1.1]{1}{0.59}
\CorrAnsButton{0.59}
\begin{solution}
0.59 \\[0.3in]
Click on that green button to return to the quiz $\to$
\end{solution}
\item\PTs{1} The probability that one computer fails  (2d.p.)\\
\RespBoxTxt{0}{0}[b1.2]{1}{0.33}
\CorrAnsButton{0.33}
\begin{solution}
0.33 \\[0.3in]
Click on that green button to return to the quiz $\to$
\end{solution}

\item\PTs{1} The probability that two computers fail  (2d.p.)\\
\RespBoxTxt{0}{0}[b1.3]{1}{0.07}
\CorrAnsButton{0.07}
\begin{solution}
0.07 \\[0.3in]
Click on that green button to return to the quiz $\to$
\end{solution}

\item\PTs{1} The probability that three computers fail  (2d.p.)\\
\RespBoxTxt{0}{0}[b1.4]{1}{0.01}
\CorrAnsButton{0.01}
\begin{solution}
0.01 \\[0.3in]
Click on that green button to return to the quiz $\to$
\end{solution}

\item\PTs{1} The probability that four computers fail  (2d.p.)\\
\RespBoxTxt{0}{0}[b1.5]{1}{0.00}
\CorrAnsButton{0.00}
\begin{solution}
0.00
\end{solution}

\item\PTs{1} The probability that five computers fail (2d.p.)\\
\RespBoxTxt{0}{0}[b1.6]{1}{0.00}
\CorrAnsButton{0.00}
\begin{solution}
0.00 \\[0.3in]
Click on that green button to return to the quiz $\to$
\end{solution}

\end{questions}
\end{quiz}\quad\eqButton{binomial}Points: \ScoreField{binomial}

This density function will be plotted on the next page, along with a $X \sim B(5, 0.5)$, $X \sim B(100, 0.1)$ and $X \sim B(100, 0.5)$.


\newpage


\begin{figure}
<<bin, fig = TRUE, echo = FALSE, results= hide>>=
par(las = 1)
x <- barplot(dbinom(c(0:6),6,0.5), col = "hotpink", main = "Illustration of Binomial(n=6,p=0.5)", xlab = "x", ylim = c(0,0.4))
axis(1,x,c(0:6))
text(x,dbinom(c(0:6),6,0.5),round(dbinom(c(0:6),6,0.5),3), pos = 3)
@
\end{figure}

\newpage

\subsection{The Geometric Distribution}

{\color{green} By convention we write $X \sim Go(\pi)$} 

We shall briefly consider the scenario whereby successive Bernoulli trials are continued until the first success occurs.   Let $X$ denote the number of trials up to but not including the first success.   This gives rise to what we call the Geometric distribution.

\begin{df}
For data assumed to follow the Geometric distribution, $X$ has a probability density function:
\begin{displaymath}
p[X=x] = (1-\pi)^{x}\pi; x=1,2,\ldots; 0 \leq \pi \leq 1
\end{displaymath}
\end{df}

\newpage

It is quite simple to derive this pmf.   We have (for example):
\begin{tabular}{rrrrr}
F & F & F & F & S \\
$q$ & $q$ & $q$ & $q$ & $p$
\end{tabular}

There is clearly only one way in which we can obtain this set of events.

It is clearly non-negative, to check it sums to one we need:

\begin{displaymath}
\sum_{x=0}^{\infty} pq^x = p \sum_{x=0}^{\infty} q^x
\end{displaymath}
That term is a standard geometric series which we can write also as $\frac{1}{1-q}$.   So we have 


\begin{displaymath}
\sum_{x=0}^{\infty} pq^x = \frac{p}{1-q} = \frac{p}{p} = 1
\end{displaymath}

\newpage

A little more work is needed to obtain the expectation.

\begin{displaymath}
E[X] = \sum_{x=0}^{\infty} x p q^x =  p \sum_{x=1}^{\infty} x q^x
\end{displaymath}
Note the trick of removing $x=0$ as the sequence is zero at that point.   This looks a little, but not entirely like a geometric series.   Let's do some maninpulation.  On line 2 we have differentiated both sides, on row three we have multiplied both sides by $a$:

\begin{eqnarray*}
\sum_{b=0}^{\infty} a^b &=& \frac{1}{1-a} \\
\sum_{b=0}^{\infty} b a^{b-1} &=& \frac{1}{(1-a)^2} \\
\sum_{b=0}^{\infty} b a^{b} &=& \frac{a}{(1-a)^2} \\
\end{eqnarray*}

So this gives us the expression we need.

\begin{displaymath}
 E[X] = p \sum_{x=1}^{\infty} x q^x = p \frac{q}{(1-q)^2} = \frac{pq}{p^2} = \frac{q}{p}
\end{displaymath}

\newpage

Note carefully.   You can also define the Geometric as the number number of trials, \emph{including} the first success.   In that case we have:

\begin{displaymath}
P[X=x] = q^{x-1}p
\end{displaymath}

and (no proof):

\fbox{Note that $E[X] = \frac{1}{\pi}$ and $V[X]=\frac{1-\pi}{\pi^2}$.}


\newpage
  
For example, if a salesperson wishes to calculate the probability that they will make their first successful sale of the day to the fifth customer, given that the probability of a sale $\pi=0.2$ is given by $p[X=5]=(1-0.2)^{5-1}0.2 = 0.08192$

We can also define $X$ to be the number of trials preceding the first success, in which case the probability density function is given by:
\begin{displaymath}
p[X=x]=(1-\pi)^x\pi
\end{displaymath}

For example, a burglar want to know that he will get away with three ``jobs'' before he finally gets caught.  Assuming the probability of being caught (a success for the rest of us) is $\pi=0.6$ we have $p[X=3]=(1-0.6)^30.6=0.0384$



\newpage

\subsection{The negative binomial distribution}

{\color{green}By convention we would write $X \sim NB(r,\pi)$}

If we extend the previous idea so that we can continue the Bernoulli trials until the $r$th success, and denote $X$ as the number of such trials required, then $X$ has a negative binomial distribution.


\begin{df}
The negative binomial distribution is given by:
\begin{displaymath}
p[X=x]={x-1 \choose r-1} \pi^r (1-\pi)^{x-r}
\end{displaymath}
\end{df}

\fbox{  Note that $E[X]=\frac{r}{\pi}$ and $V[X]=\frac{r(1-\pi)}{\pi^2}$}

\newpage

Consider a country where the prime minister has an interesting private life   The probability that a person believes a rumour about this prime minister is $\pi=0.25$.  


\begin{itemize} 
\item What are the probabilities that the sixth person to hear the rumour will be the sixth to believe it: $p[X=6|r=6,\pi=0.25] = {(6-1) \choose (6-1)} 0.25^6 (1-0.25)^{6-6}$ which should equal $0.00024$
\item What is the probability that the twelfth person to hear the rumour will be the fourth to believe it: $p[X=4|r=12,\pi=0.25] = {(12-1) \choose (4-1)} 0.25^4 (1-0.25)^{12-4}$ which should equal $0.065$
\end{itemize}

\newpage

\subsection{The Hypergeometric distribution}

This isn't what happens to a geometric distribution if you give it too many sugary sweets and fizzy drinks.   A random variable $X$ is said to follow the hypergeometric distribution if its probability mass function (pmf) is given by:

$$P(X=k) = {{{K \choose k} {{N-K} \choose {n-k}}}\over {N \choose n}}$$

Where:
\begin{itemize}
\item    $N$ is the population size
\item $K$ is the number of "successes" in the population
\item $n$ is the sample size
\item $k$ is the number of successes in the sample.
\end{itemize}
Do note that ${a \choose b}$ is a binomial coefficient for integers $a$ and $b$.


Showing that this is a valid p.m.f. - see tutorial sheets.

\newpage


\subsection{The Poisson distribution}

{\color{green}By convention we would write $X \sim P(\lambda)$} 

The Poisson distribution is used to model events occurring at random in time, area, volume.

\begin{df}
The Poisson distribution, with parameter $\lambda$ is given as follows:
\begin{displaymath}
p[X=x] = \frac{\lambda^x e^{-\lambda}}{x!};x=0,1,2,\ldots; 0 \leq \lambda \leq \infty
\end{displaymath}
\end{df}

\newpage

You can check it's non-negative.   What about summing to one:

\begin{displaymath}
\sum_{x=0}^{\infty} \frac{e^{-\lambda}\lambda^x}{x!} = e^{-\lambda} \sum_{x=0}^{\infty} \frac{\lambda^x}{x!}
\end{displaymath}

You should check that the right hand part here is the Taylor series for $e^{\lambda}$ so we have
\begin{displaymath}
\sum_{x=0}^{\infty} \frac{e^{-\lambda}\lambda^x}{x!} = e^{-\lambda}e^{\lambda} = 1
\end{displaymath}

\newpage

A little bit more work is needed to find the expecation.

\begin{displaymath}
E[X] = \sum_{x=0}^{\infty} x \frac{e^{-\lambda}\lambda^x}{x!} = e^{-\lambda} \sum_{x=0}^{\infty} x \frac{\lambda^x}{x!}
\end{displaymath}

We can repeat our trick of starting the summation from $x=1$, and note that in $\frac{x}{x!}$ we can cancel one $x$ giving us $\frac{1}{(x-1)!}$

\begin{displaymath}
E[X]  = e^{-\lambda} \sum_{x=1}^{\infty}  \frac{\lambda^x}{(x-1)!}
\end{displaymath}

We can also bring one $\lambda$ into the front of the sum

\begin{displaymath}
E[X]  = \lambda e^{-\lambda} \sum_{x=1}^{\infty}  \frac{\lambda^{(x-1)}}{(x-1)!}
\end{displaymath}

We can now do some change-of-variables: $j=k-1$ but you should be able to see where this is going.   The rightmost term is our Taylor series for $e^{\lambda}$ again.   So that:

\begin{displaymath}
E[X] = \lambda e^{-\lambda} e^{\lambda} = \lambda
\end{displaymath} 

We will though find a much simpler way to get $Var(X)$




\fbox{Summary: $E[X]=\lambda$ and $V[X]=\lambda$.}

\newpage

The number of accidents on the Pasadena Freeway between 9am and 10am on a Sunday morning is a random variable, thought to follow a Poisson distribution with $\lambda = 7.3$.   Find the probability that there will be:

\begin{itemize}
\item Exactly 6 accidents.\\ For this we need to find $p[X=6|\lambda=7.3]$ which is given by $ \frac{7.3^6e^{-7.3}}{6!} = 0.1420$
\item Less than 4 accidents.\\ For this we need the distribution which is given by  $p[X < 4 |\lambda=7.3]$.   We find this by taking the following\\ $ p[X =0 |\lambda=7.3] + p[X = 1 |\lambda=7.3] + p[X =2  |\lambda=7.3] + p[X =3|\lambda=7.3]$.   This is given by calculating\\ $\frac{7.3^0e^{-7.3}}{0!} + \frac{7.3^1e^{-7.3}}{1!} + \frac{7.3^2e^{-7.3}}{2!} + \frac{7.3^3e^{-7.3}}{3!}$ which should give $0.0674$
\end{itemize}



\newpage

These are the most important examples of discrete probability functions (although there are hundreds available).   It is worth being familiar with the key properties of these densities.

\NaviBarOff
\end{document}